{"componentChunkName":"component---src-components-blog-layout-js","path":"/blog/2021-02-04-transformers/","result":{"data":{"mdx":{"id":"93c4fb06-e1a6-5707-9cc2-e5bea7cdcd5a","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"The Transformer Architecture\",\n  \"date\": \"2021-02-04T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"The \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://arxiv.org/abs/1706.03762\"\n  }), \"Attention Is All You Need\"), \" paper in 2017 introduced the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Transformer\"), \" architecture for neural networks. This architecture does not use any recurrent or convolutional layers. It is comprised entirely of linear layers, attention and normalization layers. We now see a widespread success of this architectures. Chances are high that you've heard of BERT, if not used pre-trained models of it.\"), mdx(\"p\", null, \"Just like in a \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://ravimashru.dev/blog/2021-01-28-convolutional-seq2seq/\"\n  }), \"convolutional sequence-to-sequence model\"), \", transformers have an encoder and a decoder.  The input to the model in this case is also input sequence tokens and their positions. The positions are required because, like the convolutional seq2seq model, there is no recurrent layer that knows about the order of the tokens in the input sequence.\"), mdx(\"h2\", null, \"The Encoder\"), mdx(\"p\", null, \"In the encoder, the input is first passed through a multi-head attention layer. Since the attention layer calculates attention over the input sequence itself, it is called \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"self attention\"), \". A residual connection exists between this layer's output and the original input sequence. The output is then passed to a normalization layer. Finally, the output of that layer is passed to a position-wise feedforward layer and then through another normalization layer.\"), mdx(\"h3\", null, \"Multi-Head Attention Layer\"), mdx(\"p\", null, \"The multi-head attention layer is responsible for calculating and applying attention using three concepts: queries, keys and values. The query and key vectors provide an attention vector, which is then used to get a weighted sum of elements in the value vectors.\"), mdx(\"p\", null, \"The input to this layer is passed through three different linear layers, one for \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"queries\"), \", \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"keys\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"values\"), \" each. The output of each of these layers is then split into a fixed number of contiguous elements, each of which is called a head (hence the name \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"multi-head\"), \").\"), mdx(\"p\", null, \"The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"query\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"key\"), \" vectors of each head are multiplied and passed through a softmax layer to get the attention values.\"), mdx(\"p\", null, \"The attention vector for each head is multiplied with the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"value\"), \" vectors. The multiple heads in the resulting vector is combined and the output is passed through a final linear layer. The result is then passed to a position-wise feedforward layer.\"), mdx(\"h3\", null, \"Position-wise Feedforward Layer\"), mdx(\"p\", null, \"The position feedforward layer is a combination of two linear layers. The first layer transforms the hidden dimension of the input to a much larger dimension. An activation function (e.g. ReLU or GELU) is applied to the output and passed to the second layer.\"), mdx(\"p\", null, \"The second layer does the exact opposite - transforms the hidden dimension of the vectors back to its original value.\"), mdx(\"h3\", null, \"Normalization Layers\"), mdx(\"p\", null, \"The normalization layers are used to ensure each feature has a mean of 0 and standard deviation of 1. This makes training large networks easier.\"), mdx(\"h2\", null, \"The Decoder\"), mdx(\"p\", null, \"The decoder consists of blocks of two multi-head attention layers and a position-wise feedforward layer with normalization layers in between.\"), mdx(\"h3\", null, \"The First Multi-head Attention Layer\"), mdx(\"p\", null, \"The first multi-head attention layer (called the masked multi-head attention layer, because it uses a target mask token) performs self attention like the layer in the encoder, but using the decoder representation as the query, key and value vectors. This is followed by a residual connection and a normalization layer like in the encoder.\"), mdx(\"h3\", null, \"Target Mask Token\"), mdx(\"p\", null, \"Since all tokens are processed at once in the decoder, a special padding token is used to ensure the decoder can only see the tokens that come before the one it is trying to predict.\"), mdx(\"h3\", null, \"The Second Multi-head Attention Layer\"), mdx(\"p\", null, \"The second multi-head attention layer uses the decoder representation as queries, and the output of the encoder as keys and values. This layer is also followed by a residual connection and normalization layer.\"), mdx(\"p\", null, \"The output is passed through a position-wise feedforward network that works in the same way as the one in the encoder, and finally through a final normalization layer.\"), mdx(\"h2\", null, \"Further Reading\"), mdx(\"p\", null, \"A more detailed explanation of these concepts and complete code to train such a model is available in \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\"\n  }), \"this notebook\"), \" by \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/bentrevett\"\n  }), \"@bentrevett\"), \". There are a lot of other amazing tutorials in his GitHub repositories. Be sure to check them out!\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"The Transformer Architecture"}}},"pageContext":{"id":"93c4fb06-e1a6-5707-9cc2-e5bea7cdcd5a"}},"staticQueryHashes":["3069025275","63159454"]}