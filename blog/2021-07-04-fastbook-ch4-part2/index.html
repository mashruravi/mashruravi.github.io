<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.1db540ecee05b1203c9d.css" id="gatsby-global-css">@import url(https://fonts.googleapis.com/css?family=Raleway|Roboto&display=swap);@font-face{font-family:Socialico;src:url(/static/Socialico-Regular-1fd9a7b1ba4a914c010b8a8ff0636c61.woff)}@font-face{font-family:SocialicoPlus;src:url(/static/SocialicoPlus-Plus-36c8776ee66b40b4175cfc1bb94e275f.woff)}body{margin:0;-moz-osx-font-smoothing:grayscale;font-family:Roboto,Helvetica,Arial,sans-serif;-webkit-font-smoothing:antialiased}header{height:48px;display:flex;justify-content:center;align-items:center;padding:0 16px;background-color:rgba(0,0,0,.8);-webkit-backdrop-filter:saturate(180%) blur(20px);backdrop-filter:saturate(180%) blur(20px);transition:background-color .5s}header.expanded{background-color:#000}.hdr-normal{display:block}.hdr-col{display:none}.header-link{color:#fff;text-decoration:none;font-family:Raleway;margin:0 18px;padding:8px 16px;border-radius:10px}.prf-hdr-grid-ctr{display:flex}.prf-hdr-ctr{color:#6c6c6c;font-size:36px;margin:36px auto;display:grid;grid-template-columns:minmax(250px,25%) auto}.prf-hdr-intro>div{line-height:36px;margin-bottom:16px;margin-top:16px}.prf-hdr-intro strong{color:#19b6ff}.prf-hdr-img{margin:auto;grid-row-start:1;grid-row-end:3}.prf-hdr-social{display:flex}.hdr-col-menu{width:50px;height:40px;position:relative;cursor:pointer}.hdr-col-menu-bar{position:absolute;height:1px;background-color:#fff;left:12px;right:12px}.hdr-col-menu-bar-1{top:14px;transition:top .25s ease-out .15s,transform .15s ease-out}.hdr-col-menu-bar-2{bottom:14px;transition:bottom .25s ease-out .15s,transform .15s ease-out}.hdr-col-menu-bar-1-open{top:19px;transform:rotate(45deg);transition:top .15s ease-out,transform .25s ease-out .15s}.hdr-col-menu-bar-2-open{bottom:19px;transform:rotate(-45deg);transition:bottom .15s ease-out,transform .25s ease-out .15s}.hdr-col-itm-ctr{background-color:#000;position:absolute;left:0;right:0;top:48px;height:0;transition:.35s ease-out;overflow:hidden}.hdr-col-itm-ctr-exp{height:calc(100vh - 48px)}.hdr-col-itm-ctr-inner{display:flex;flex-direction:column;align-items:center;opacity:0;transition:opacity .25s ease-out .1s}.hdr-col-itm-ctr-inner-exp{opacity:1}.header-link-col{font-size:36px;height:48px;text-decoration:none;margin:16px 0;display:flex;align-items:center;color:#fff;padding:8px 24px;border-radius:10px}.section-container{margin:16px 64px;padding:32px 0}.section-header{color:#6c6c6c;font-size:32px;font-weight:700;text-align:center;margin:16px 0}.section-body{display:flex;flex-wrap:wrap;justify-content:center;margin:0 auto}.tech-item{display:flex;flex-direction:column;justify-content:flex-start;align-items:center;box-sizing:border-box;padding:16px;width:180px}.tech-item>img{height:60px}.tech-item>span{color:#6c6c6c;margin:8px 0}.proj-item{width:500px;height:380px;box-shadow:0 0 8px 0 rgba(0,0,0,.2),0 0 50px 0 rgba(0,0,0,.09);margin:24px;color:#6c6c6c;display:flex;flex-direction:column;box-sizing:border-box;position:relative;cursor:pointer;transition:box-shadow .2s ease-out}.proj-item:hover{box-shadow:0 0 16px 0 rgba(0,0,0,.2),0 0 50px 0 rgba(0,0,0,.09)}.proj-item:hover .proj-item-overlay{opacity:1}.proj-item-image,.proj-item-image-ctr{width:100%;height:100%}.proj-item-image{-o-object-fit:cover;object-fit:cover}.proj-item-overlay{opacity:0;padding:16px;position:absolute;background-color:rgba(0,0,0,.8);color:#fff;top:0;bottom:0;left:0;right:0;transition:opacity .5s}.proj-item-title{font-size:24px;font-weight:700;margin:24px 0;text-align:center}.proj-item-desc{font-size:24px;margin:0 16px;text-align:center}.proj-item-spacer{margin:auto}footer{padding:48px 16px}.blog-header{color:#6c6c6c;font-size:36px;text-align:center}.blog-list{max-width:650px;margin:0 auto;padding:0 16px}.blog-list-item{display:flex;flex-direction:column;margin:24px 0;padding:16px;color:#6c6c6c;text-decoration:none;transition:color .15s linear,border .15s linear;font-size:18px;box-sizing:border-box;border:1px solid transparent}.blog-list-item span:nth-child(2){font-size:12px}.blog-list-item:hover{color:#0c0c0c;border:1px solid rgba(0,0,0,.3)}.blog-content{padding:0 16px;max-width:800px;margin:0 auto;line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,Roboto,Helvetica Neue,Arial,sans-serif}.blog-content blockquote{background:rgba(0,0,0,.03);padding:8px 32px;white-space:break-spaces;word-wrap:break-word}.blog-content h1{color:#6c6c6c}.blog-body{margin-bottom:36px;text-align:justify}.blog-body code{background-color:#d3d3d3;padding:4px 6px}.blog-body pre{background:#d3d3d3;padding:24px 12px;overflow:scroll;font-size:1.2rem}.blog-body pre code{padding:0}@media only screen and (max-width:750px){.prf-hdr-ctr{grid-template-columns:100%;font-size:24px;margin:16px;text-align:center;width:100%}.prf-hdr-intro>div{line-height:24px;margin-bottom:16px}.prf-hdr-img{margin:24px auto}.prf-hdr-social{margin:0 auto}.hdr-normal{display:none}header{justify-content:flex-end}.hdr-col{display:block}.section-container{margin:64px 8px;padding:0}.tech-item{width:110px}.tech-item>img{height:36px}.tech-item>span{text-align:center}.proj-item{width:300px;height:220px;margin:24px}.proj-item-title{font-size:22px;margin:24px 0;text-align:center}.proj-item-desc{font-size:16px}footer{padding:24px 8px}.blog-header{text-align:left;padding:0 16px}.blog-content h1{text-align:center}}@media only screen and (min-width:751px) and (max-width:900px){.prf-hdr-ctr{font-size:26px}.section-container{margin:32px 16px}.proj-item{width:300px;height:220px;margin:24px}.proj-item-title{font-size:22px;margin:24px 0;text-align:center}.proj-item-desc{font-size:16px}footer{padding:48px 16px}.blog-header{text-align:center}}.social-icon{margin:0 8px;font-size:64px}.social-icon>a{text-decoration:none;color:#6c6c6c;transition:.15s linear}.si{font-family:Socialico}.sip{font-family:SocialicoPlus}.si-linkedin>a:hover{color:#0077b5}.si-twitter>a:hover{color:#1ea1f2}.si-mail>a:hover{color:red}.si-github>a:hover{color:#000}</style><meta name="generator" content="Gatsby 2.31.1"/><title data-react-helmet="true">Reading Notes - Ch. 4 - Fastbook (part 2) - Blog - Ravi Mashru</title><meta data-react-helmet="true" name="description" content="Ravi Mashru&#x27;s home page on the internet - personal portfolio, blog and notes."/><meta data-react-helmet="true" property="og:title" content="Reading Notes - Ch. 4 - Fastbook (part 2) - Blog"/><meta data-react-helmet="true" property="og:description" content="Ravi Mashru&#x27;s home page on the internet - personal portfolio, blog and notes."/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:creator" content="@mashruravi"/><meta data-react-helmet="true" name="twitter:title" content="Reading Notes - Ch. 4 - Fastbook (part 2) - Blog"/><meta data-react-helmet="true" name="twitter:description" content="Ravi Mashru&#x27;s home page on the internet - personal portfolio, blog and notes."/><link rel="icon" href="/favicon-32x32.png?v=86476019a66cd05dc3116d3ffde5ff93" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><meta name="theme-color" content="#323232"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=86476019a66cd05dc3116d3ffde5ff93"/><link as="script" rel="preload" href="/webpack-runtime-3acf2310e2b150f8f965.js"/><link as="script" rel="preload" href="/framework-741ade27086b2708e961.js"/><link as="script" rel="preload" href="/app-4bb33029cc1bf54f0b6b.js"/><link as="script" rel="preload" href="/styles-e9d24b1846c7d6eb9685.js"/><link as="script" rel="preload" href="/commons-69d67839b5d44cb35357.js"/><link as="script" rel="preload" href="/component---src-components-blog-layout-js-31e6c84f998402f3e848.js"/><link as="fetch" rel="preload" href="/page-data/blog/2021-07-04-fastbook-ch4-part2/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3069025275.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/63159454.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div style="position:fixed;top:0;left:0;right:0;z-index:1"><header class=""><div class="hdr-normal"><a class="header-link" href="/">Home</a><a class="header-link" href="/blog/">Blog</a><a class="header-link" href="https://github.com/mashruravi/notes" target="_blank" rel="noopener noreferrer">Notes</a></div><div class="hdr-col"><div class="hdr-col-menu"><div class="hdr-col-menu-bar hdr-col-menu-bar-1"></div><div class="hdr-col-menu-bar hdr-col-menu-bar-2"></div></div><div class="hdr-col-itm-ctr "><div class="hdr-col-itm-ctr-inner "><a class="header-link-col" href="/">Home</a><a class="header-link-col" href="/blog/">Blog</a><a class="header-link-col" href="https://github.com/mashruravi/notes" target="_blank" rel="noopener noreferrer">Notes</a></div></div></div></header></div><main style="padding-top:98px;min-width:350px;margin:auto;min-height:calc(100vh - 98px - 50px)"><div class="blog-content"><h1>Reading Notes - Ch. 4 - Fastbook (part 2)</h1><div class="blog-body"><main><h2>Gradient Descent</h2><p>Recall: Arthur Samuel&#x27;s description of machine learning</p><blockquote><p> Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.</p></blockquote><p>Gradient descent is the &quot;mechanism&quot; that we use to alter the weight assignment (i.e. parameters of our model) so that we can maximize its performance (i.e. minimize the loss value).</p><p>The following diagram shows the different components involved in training a deep learning model:</p><div class=" gatsby-image-wrapper" style="position:relative;overflow:hidden;display:block;width:100%;height:299px"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAzklEQVQoz52S2QqFMAxE+///6JuKFhT3XXM5gVxqQRAHhmrTTJJp3bZtsu/7jewNwyDTNEmSJJKmqSzLIuM43s4dx6Fn4Xmecl2XuLIspaoqTUAAIgbbtlWxoig0qes63Z/nWcVZiVkcOFRhCP5JsAKQZIrGoFNilvcXjPmEOM7YFDQ4+Qi6MvZ9L+u66lSurmv10HwJx3wiAhA/8zyXLMvU36ZpxGEm6rT+lviGMA0gQkN8U+TzyKGn4WU9XsobxoJ6y1+7stV7r28VG8APQFERR8fT7u8AAAAASUVORK5CYII=" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:contain;object-position:center;opacity:1;transition-delay:500ms;margin:0 auto"/><noscript><picture><source srcset="/static/a44edd55545232d9bdd0c116d2061a7b/2a4de/dl-tr.png 1x,
/static/a44edd55545232d9bdd0c116d2061a7b/db955/dl-tr.png 1.5x,
/static/a44edd55545232d9bdd0c116d2061a7b/f3583/dl-tr.png 2x" /><img loading="lazy" width="600" height="299" srcset="/static/a44edd55545232d9bdd0c116d2061a7b/2a4de/dl-tr.png 1x,
/static/a44edd55545232d9bdd0c116d2061a7b/db955/dl-tr.png 1.5x,
/static/a44edd55545232d9bdd0c116d2061a7b/f3583/dl-tr.png 2x" src="/static/a44edd55545232d9bdd0c116d2061a7b/2a4de/dl-tr.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div><p>In this setup, we can use calculus to calculate how we should update the parameters of the model so that the loss value decreases. This change we have to make in the parameters of the model is given by the <strong>gradient</strong> of the loss function with respect to the parameters.</p><p>Technically, the gradient specifies the direction of steepest <strong>increase</strong> of the loss function. So we update the parameters in the negative direction of the gradient - i.e. the steepest <strong>decrease</strong>.</p><div class=" gatsby-image-wrapper" style="position:relative;overflow:hidden;display:block;width:100%;height:356px"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABBUlEQVQoz51TXY+EIAz0//9HX9Qn4oq6iorg3A6XmiprLrkmjdROpx+U4jgOUGOMScX+rxb4SAgBdVWh+mjf9wqARzkenIUEkzTGAOcW+BDh/X4GatWEzrkMU9wzve0Lb9NgGmxWiSb13mNZlgxztjyOY9Ledgirw/gyJ/BblSQjaUYoBsmGYcA0TeCfeZ6xrmtWmdj064QZoYgAWTXPOuCvdi+XIg6ScH0oHPq2bVkguxCMjr1ciia8J9j3/dKBtu9rlBFylgz4XaOYzjJfKqumj23LQ9CkXys0xqCuazRNg67r0uWwTZLxzC9nSFxZlglnrc338D4nmd+TMHnbtomM1TP+B1gbsZxmTWzKAAAAAElFTkSuQmCC" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:contain;object-position:center;opacity:1;transition-delay:500ms;margin:0 auto"/><noscript><picture><source srcset="/static/6abec0d73b52bab3dc503d6cfca488a7/2a4de/gd.png 1x,
/static/6abec0d73b52bab3dc503d6cfca488a7/db955/gd.png 1.5x,
/static/6abec0d73b52bab3dc503d6cfca488a7/f3583/gd.png 2x" /><img loading="lazy" width="600" height="356" srcset="/static/6abec0d73b52bab3dc503d6cfca488a7/2a4de/gd.png 1x,
/static/6abec0d73b52bab3dc503d6cfca488a7/db955/gd.png 1.5x,
/static/6abec0d73b52bab3dc503d6cfca488a7/f3583/gd.png 2x" src="/static/6abec0d73b52bab3dc503d6cfca488a7/2a4de/gd.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div><p>After finding the gradient, we adjust the parameters of the model in the direction specified by the gradient to reduce the loss value, hence the name <strong>gradient descent</strong>.</p><div class=" gatsby-image-wrapper" style="position:relative;overflow:hidden;display:block;width:100%;height:137px"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAoklEQVQY03VQ3QqGIBTz/R8x6kqILOlPrYxqHxt48UENhjtyzpzHPM+DLx7HgRACYozS27ZhXVfVKaXXGfN2SVzXhWVZ0Pc9hmGQyTzP6LpO9N7jvm/1lhnC4APnecr0CzTZ9/3PTAmZoGka1HWtJHyZuqoqJaIxybTTNEnnnLWKtm1hrcU4jqrZbyicc2LZDY1ZlwT8WjGmJmnKXpJ93DHPH+jVhLHtXEHxAAAAAElFTkSuQmCC" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:contain;object-position:center;opacity:1;transition-delay:500ms;margin:0 auto"/><noscript><picture><source srcset="/static/5beb2f91297a76509357281900e45229/2a4de/gd-flowchart.png 1x,
/static/5beb2f91297a76509357281900e45229/db955/gd-flowchart.png 1.5x,
/static/5beb2f91297a76509357281900e45229/f3583/gd-flowchart.png 2x" /><img loading="lazy" width="600" height="137" srcset="/static/5beb2f91297a76509357281900e45229/2a4de/gd-flowchart.png 1x,
/static/5beb2f91297a76509357281900e45229/db955/gd-flowchart.png 1.5x,
/static/5beb2f91297a76509357281900e45229/f3583/gd-flowchart.png 2x" src="/static/5beb2f91297a76509357281900e45229/2a4de/gd-flowchart.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div><p>We have a way of improving the weights using gradient descent so it&#x27;s completely fine to start with randomly initialized weights.</p><p>We stop training the model when we see that the accuracy of the model starts getting worse or we run out of time.</p><h2>Batch, Mini-batch and Stochastic Gradient Descent</h2><p>When calculating the gradient of the loss function with respect to the model parameters, at each step you can use all the data in the training set (batch gradient descent), one data point from the training set (stochastic gradient descent), or a subset of data (i.e. a mini-batch) in the training set (mini-batch gradient descent).</p><p>Batch gradient descent gives the most accurate gradient. However, computing the gradient over the entire dataset takes a long time. Also, it may not be possible to fit all your data in memory (especially a GPU) to use batch gradient descent if you have a very large dataset.</p><p>Stochastic gradient descent is the fastest since the gradient has to be calculated using only one point. However, it gives a very imprecise and unstable gradient. Also, using one data point at a time doesn&#x27;t allow you to make full use of the parallel processing capabilities of a GPU.</p><p>Mini-batch gradient descent is a compromise between batch and stochastic gradient descent. A larger batch size (number of items in a mini-batch) gives a more accurate and stable estimate of the gradient, however smaller batches are faster to compute and allows you to update the gradient more number of times in an epoch.</p><p>Note: randomly shuffling items in mini-batches after every epoch can result in better generalization.</p><h2>Learning Rate</h2><p>The learning rate determines how big of a step we take when updating the weights. If it is too small, you will have to train the model for longer to reach the minima of the loss function. However, if it is too high then the loss will keep overshooting the minima. In fact, if the learning rate is sufficiently high, the loss can even start increasing.</p><p>The learning rate is used to control the weight update as follows:</p><p><code>New parameters = Old parameters - (gradient x learning rate)</code></p><h2>The Loss Function</h2><p>The choice of loss function is important for the gradient descent process to work. The loss function needs to update even when we make slight adjustments to our model parameters.</p><p>This is why accuracy is not a good choice for a loss function. If we change our weights a little bit, the output of the model may not change significantly and therefore its accuracy may not change. This will result in a gradient value of zero and our model won&#x27;t be able to learn at that step.</p><h2>Nonlinearities</h2><p>A linear classifier is one whose output is a function of the linear combination of the model&#x27;s parameters and input features. Adding multiple linear layers one after another can&#x27;t improve the performance of a model because multiple linear layers, one after another, are equivalent to a single linear layer (with different parameters).</p><p>Adding a nonlinearity (also called an &quot;activation function&quot;) between linear layers decouples the layers and enables each layer to learn on its own.</p><p>Two examples of nonlinearities are the ReLU and Sigmoid functions</p><div class=" gatsby-image-wrapper" style="position:relative;overflow:hidden;display:block;width:100%;height:256px"><img aria-hidden="true" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA3ElEQVQoz22SWQ6EMAxDe/97gvhgEftOZl4ldwKMpaitcRMnJdgXx3FY13VxBdd1pdDZA11VVVaW5e0OCBBEnue2rusrgfbnedq+74kjIeGLEgFnHhJwWdXBPM9WFEXkt22LBdDC933/c7gsS3KgKtM02TAMr5bh0PlkGKJISgiphIBqtPLPMZc5KynaZ3fJIaAViZ4PA88jaBR1XZvMeF1AxGOM4xjjCS8mAXv0fiQ3h03TxI+sbdvG+RE4YvXusyyLLqWVnvvqLGjAVMWB5qOzoN9GekaFRry6+wC7X8NZikTmkwAAAABJRU5ErkJggg==" alt="" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:contain;object-position:center;opacity:1;transition-delay:500ms;margin:0 auto"/><noscript><picture><source srcset="/static/9128640dfe8c2508652ebe0776891a34/2a4de/non-linearities.png 1x,
/static/9128640dfe8c2508652ebe0776891a34/db955/non-linearities.png 1.5x,
/static/9128640dfe8c2508652ebe0776891a34/f3583/non-linearities.png 2x" /><img loading="lazy" width="600" height="256" srcset="/static/9128640dfe8c2508652ebe0776891a34/2a4de/non-linearities.png 1x,
/static/9128640dfe8c2508652ebe0776891a34/db955/non-linearities.png 1.5x,
/static/9128640dfe8c2508652ebe0776891a34/f3583/non-linearities.png 2x" src="/static/9128640dfe8c2508652ebe0776891a34/2a4de/non-linearities.png" alt="" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div><h2>The Universal Approximation Theorem</h2><blockquote><p>For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it close to the wiggly function, we just have to use shorter lines.</p></blockquote><h2>Deeper Models</h2><p>We can build deep models using multiple linear layers with a nonlinearity between them. However, the deeper a model gets, the harder it is to optimize its parameters in practice.</p><p>The advantage of using deep models is that you can use a deeper model with fewer parameters to get the same performance as a shallower model with more parameters.</p><p>As a result, deeper models can be trained faster and using less memory.</p></main></div></div></main><footer style="background-color:#6C6C6C;color:#FFF;font-size:12px"><div style="max-width:1100px;min-width:300px;margin:0 auto">© <!-- -->2021<!-- --> Ravi Suresh Mashru. All rights reserved.</div></footer></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/blog/2021-07-04-fastbook-ch4-part2/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2966d4d8fea208fd6f8f.js"],"app":["/app-4bb33029cc1bf54f0b6b.js"],"component---src-components-blog-layout-js":["/component---src-components-blog-layout-js-31e6c84f998402f3e848.js"],"component---src-pages-404-js":["/component---src-pages-404-js-689cfc1b5c0a2e8a2c2e.js"],"component---src-pages-blog-js":["/component---src-pages-blog-js-806e931df0ec1e623367.js"],"component---src-pages-index-js":["/component---src-pages-index-js-1b32ac085bafa8f65d36.js"],"component---src-pages-reading-list-js":["/component---src-pages-reading-list-js-d3c0034680be27e8676e.js"]};/*]]>*/</script><script src="/polyfill-2966d4d8fea208fd6f8f.js" nomodule=""></script><script src="/component---src-components-blog-layout-js-31e6c84f998402f3e848.js" async=""></script><script src="/commons-69d67839b5d44cb35357.js" async=""></script><script src="/styles-e9d24b1846c7d6eb9685.js" async=""></script><script src="/app-4bb33029cc1bf54f0b6b.js" async=""></script><script src="/framework-741ade27086b2708e961.js" async=""></script><script src="/webpack-runtime-3acf2310e2b150f8f965.js" async=""></script></body></html>